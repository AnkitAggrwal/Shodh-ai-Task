{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ee0cb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import d3rlpy\n",
    "from d3rlpy.algos import DiscreteCQLConfig\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "\n",
    "print(\"Libraries loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67442d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaded successfully.\n",
      "✅ Deep Learning model loaded.\n",
      "2025-10-29 22:09.40 [info     ] Signatures have been automatically determined. action_signature=Signature(dtype=[dtype('int32')], shape=[(1,)]) observation_signature=Signature(dtype=[dtype('float64')], shape=[(37,)]) reward_signature=Signature(dtype=[dtype('float64')], shape=[(1,)])\n",
      "2025-10-29 22:09.40 [info     ] Action-space has been automatically determined. action_space=<ActionSpace.DISCRETE: 2>\n",
      "2025-10-29 22:09.40 [info     ] Action size has been automatically determined. action_size=2\n",
      "✅ RL model architecture built successfully.\n",
      "✅ RL model weights loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "data = joblib.load('artifacts/preprocessed_data.joblib')\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "test_data_rl = data['test_data_rl']  # The raw test data\n",
    "preprocessor = data['preprocessor']\n",
    "\n",
    "print(\"✅ Data loaded successfully.\")\n",
    "\n",
    "# Load DL model\n",
    "dl_model = tf.keras.models.load_model('artifacts/dl_model.keras')\n",
    "print(\"✅ Deep Learning model loaded.\")\n",
    "\n",
    "# --- Build the RL model ---\n",
    "# --- Build the RL model ---\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "\n",
    "# Create configuration (discrete version)\n",
    "cql_config = DiscreteCQLConfig()\n",
    "cql = cql_config.create(device=\"cpu\")\n",
    "\n",
    "# Create a dummy dataset for initialization\n",
    "dummy_observations = np.random.rand(10, X_test.shape[1])\n",
    "dummy_actions = np.random.randint(0, 2, size=(10,))\n",
    "dummy_rewards = np.zeros(10)\n",
    "dummy_terminals = np.zeros(10)\n",
    "dummy_terminals[-1] = 1  # ✅ mark last sample as terminal (required)\n",
    "\n",
    "dummy_dataset = MDPDataset(\n",
    "    observations=dummy_observations,\n",
    "    actions=dummy_actions,\n",
    "    rewards=dummy_rewards,\n",
    "    terminals=dummy_terminals\n",
    ")\n",
    "\n",
    "# Build model architecture before loading weights\n",
    "cql.build_with_dataset(dummy_dataset)\n",
    "print(\"✅ RL model architecture built successfully.\")\n",
    "\n",
    "# Load pretrained weights\n",
    "cql.load_model('artifacts/discrete_cql_model.d3')  # or 'cql_model.d3'\n",
    "print(\"✅ RL model weights loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ee78a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559/559 ━━━━━━━━━━━━━━━━━━━━ 28s 51ms/st ━━━━━━━━━━━━━━━━━━━━ 0s 641us/st ━━━━━━━━━━━━━━━━━━━━ 0s 608us/st ━━━━━━━━━━━━━━━━━━━━ 0s 574us/st ━━━━━━━━━━━━━━━━━━━━ 0s 539us/st ━━━━━━━━━━━━━━━━━━━━ 0s 523us/st ━━━━━━━━━━━━━━━━━━━━ 0s 547us/st ━━━━━━━━━━━━━━━━━━━━ 0s 571us/step\n",
      "DL Policy: 10109 approvals, 7777 denials\n",
      "RL Policy: 17886 approvals, 0 denials\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Generate Policies from Both Models ---\n",
    "\n",
    "# Model 1: Deep Learning Policy\n",
    "# The DL model outputs a probability of default.\n",
    "# The policy is this probability + a threshold.\n",
    "# Let's use a standard 0.5 threshold.\n",
    "y_pred_prob = dl_model.predict(X_test).flatten()\n",
    "dl_policy_action = (y_pred_prob <= 0.5).astype(int) # 0 = Deny (predicts default), 1 = Approve (predicts paid)\n",
    "\n",
    "# Correction: The DL model predicts P(default=1).\n",
    "# So, prob > 0.5 means \"predicts default\".\n",
    "# The action should be \"Approve\" (1) if P(default) is LOW\n",
    "# and \"Deny\" (0) if P(default) is HIGH.\n",
    "# Let's define the policy as: \"Approve if predicted default prob < threshold\"\n",
    "THRESHOLD = 0.5 \n",
    "dl_policy_action = (y_pred_prob < THRESHOLD).astype(int) # 1 = Approve, 0 = Deny\n",
    "\n",
    "print(f\"DL Policy: {np.sum(dl_policy_action == 1)} approvals, {np.sum(dl_policy_action == 0)} denials\")\n",
    "\n",
    "# Model 2: Reinforcement Learning Policy\n",
    "# The RL agent directly outputs the optimal action (0 or 1).\n",
    "rl_policy_action = cql.predict(X_test)\n",
    "print(f\"RL Policy: {np.sum(rl_policy_action == 1)} approvals, {np.sum(rl_policy_action == 0)} denials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c2d7d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7777 disagreements out of 17886 test samples.\n",
      "\n",
      "Found 7777 cases where DL denies but RL approves.\n",
      "Here are a few examples:\n",
      "        loan_amnt  int_rate  annual_inc grade  dl_default_prob  \\\n",
      "76399     15000.0     15.59     90000.0     D         0.643804   \n",
      "143331    27950.0     18.06     69000.0     D         0.676681   \n",
      "88699     23000.0     14.64     70000.0     C         0.525788   \n",
      "50657     40000.0     11.99    220000.0     C         0.511253   \n",
      "139122    35000.0     10.42     55000.0     B         0.603592   \n",
      "23490      7850.0     16.55     38980.0     D         0.569693   \n",
      "114486    14000.0     19.20     64000.0     D         0.725955   \n",
      "115718     5000.0     17.76     60000.0     D         0.508849   \n",
      "43680     16000.0     23.32     51000.0     E         0.869769   \n",
      "9938      20000.0     13.33     48000.0     C         0.573668   \n",
      "\n",
      "        dl_policy_action  rl_policy_action  actual_outcome  \n",
      "76399                  0                 1               0  \n",
      "143331                 0                 1               0  \n",
      "88699                  0                 1               0  \n",
      "50657                  0                 1               0  \n",
      "139122                 0                 1               0  \n",
      "23490                  0                 1               0  \n",
      "114486                 0                 1               1  \n",
      "115718                 0                 1               1  \n",
      "43680                  0                 1               1  \n",
      "9938                   0                 1               0  \n"
     ]
    }
   ],
   "source": [
    "# --- 3. Find and Analyze Disagreements (Task 4.3) ---\n",
    "# This is the most important part for your report.\n",
    "\n",
    "# Add these policies to our raw test dataframe to analyze\n",
    "analysis_df = test_data_rl.copy()\n",
    "analysis_df['dl_policy_action'] = dl_policy_action\n",
    "analysis_df['rl_policy_action'] = rl_policy_action\n",
    "analysis_df['dl_default_prob'] = y_pred_prob\n",
    "analysis_df['actual_outcome'] = y_test.values\n",
    "\n",
    "# Find cases where the policies disagree\n",
    "disagreements_df = analysis_df[analysis_df['dl_policy_action'] != analysis_df['rl_policy_action']]\n",
    "\n",
    "print(f\"Found {len(disagreements_df)} disagreements out of {len(analysis_df)} test samples.\")\n",
    "\n",
    "# Let's look for the most interesting case:\n",
    "# DL model says \"Deny\" (high risk) but RL agent says \"Approve\"\n",
    "# dl_policy_action == 0 (meaning default_prob > 0.5)\n",
    "# rl_policy_action == 1\n",
    "interesting_cases = disagreements_df[\n",
    "    (disagreements_df['dl_policy_action'] == 0) & \n",
    "    (disagreements_df['rl_policy_action'] == 1)\n",
    "]\n",
    "\n",
    "print(f\"\\nFound {len(interesting_cases)} cases where DL denies but RL approves.\")\n",
    "print(\"Here are a few examples:\")\n",
    "\n",
    "# Show the key features for these cases\n",
    "print(interesting_cases[[\n",
    "    'loan_amnt', \n",
    "    'int_rate', \n",
    "    'annual_inc', \n",
    "    'grade', \n",
    "    'dl_default_prob',  # The DL model's risk score\n",
    "    'dl_policy_action', \n",
    "    'rl_policy_action',\n",
    "    'actual_outcome'    # What actually happened\n",
    "]].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
